{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ee5c18-f18b-4d2f-9ead-6df4f48a804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 11:48:02 WARN Utils: Your hostname, zurich-VivoBook-ASUSLaptop-X412FL-A412FL resolves to a loopback address: 127.0.1.1; using 192.168.1.32 instead (on interface wlo1)\n",
      "25/11/23 11:48:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/23 11:48:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/23 11:48:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/23 11:48:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.32:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70b5481b6b90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd60d726-0cc3-43e0-9d93-4e9ca0264561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/23 11:48:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/11/23 11:48:57 WARN Instrumentation: [abab7750] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/11/23 11:48:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/11/23 11:48:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "# Load sample data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272f0847-b1c8-4ff9-b7cd-5969d7685cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057946182315,4.087352272150037]\n",
      "Intercept: 11.568912743674886\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Example dataset (sesuai materi)\n",
    "data = [\n",
    "    (1, Vectors.dense([2.0, 3.0]), 0),\n",
    "    (2, Vectors.dense([1.0, 5.0]), 1),\n",
    "    (3, Vectors.dense([2.5, 4.5]), 1),\n",
    "    (4, Vectors.dense([3.0, 6.0]), 0)\n",
    "]\n",
    "\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Display coefficients and summary\n",
    "print(\"Coefficients:\", model.coefficients)\n",
    "print(\"Intercept:\", model.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a962cffe-da36-4fab-8147-c6dbfdada411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([5.33333333, 5.33333333]), array([15., 15.])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Example dataset (pakai Vectors.dense)\n",
    "data = [\n",
    "    (1, Vectors.dense([1.0, 1.0])),\n",
    "    (2, Vectors.dense([5.0, 5.0])),\n",
    "    (3, Vectors.dense([10.0, 10.0])),\n",
    "    (4, Vectors.dense([15.0, 15.0]))\n",
    "]\n",
    "\n",
    "columns = ['ID', 'Features']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train KMeans clustering model\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df)\n",
    "\n",
    "# Show cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers:\", centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e12fa1-81d6-453c-911b-a604d3df923e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset ke Spark & Persiapan Data\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"IrisClassification\").getOrCreate()\n",
    "df = spark.read.csv(\"file:///home/zurich/Downloads/Iris.csv\",\n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d762003-b775-4cb7-8a14-2e0c474567c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         Features|Label|\n",
      "+-----------------+-----+\n",
      "|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "|[4.9,3.0,1.4,0.2]|  0.0|\n",
      "|[4.7,3.2,1.3,0.2]|  0.0|\n",
      "|[4.6,3.1,1.5,0.2]|  0.0|\n",
      "|[5.0,3.6,1.4,0.2]|  0.0|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mengubah kolom label species menjadi angka\n",
    "indexer = StringIndexer(inputCol=\"Species\", outputCol=\"Label\")\n",
    "df_indexed = indexer.fit(df).transform(df)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"],\n",
    "    outputCol=\"Features\"\n",
    ")\n",
    "\n",
    "df_final = assembler.transform(df_indexed).select(\"Features\", \"Label\")\n",
    "\n",
    "df_final.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "711817c3-42c5-4a2d-80e3-282278919b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koefisien (Matrix):\n",
      "DenseMatrix([[-16.08836442,  64.64799121, -21.9737467 , -46.43057252],\n",
      "             [  9.27679647, -28.9835485 ,   6.27216491,  14.07220185],\n",
      "             [  6.81156795, -35.66444271,  15.70158179,  32.35837067]])\n",
      "\n",
      "Intercept (Vector):\n",
      "[33.325046405163036,4.656447471327328,-37.98149387649036]\n"
     ]
    }
   ],
   "source": [
    "#Membangun Model Klasifikasi\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"Features\", labelCol=\"Label\")\n",
    "model = lr.fit(df_final)\n",
    "print(\"Koefisien (Matrix):\")\n",
    "print(model.coefficientMatrix)\n",
    "\n",
    "print(\"\\nIntercept (Vector):\")\n",
    "print(model.interceptVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d01ef13-54f5-44f1-a6ff-aa94b79af52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi Model: 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "#Evaluasi Model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "pred = model.transform(df_final)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "print(\"Akurasi Model:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94b2233a-e55b-499e-9d4b-246f708b59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning (Cross-Validation)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .build())\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(df_final)\n",
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b28a708-c96a-494c-ba15-57b8bafd5557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Terbaik - regParam: 0.01\n",
      "Model Terbaik - elasticNetParam: 0.5\n",
      "Akurasi Terbaik Setelah Tuning: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "#Menampilkan model terbaik beserta tingkat Akurasi\n",
    "print(\"Model Terbaik - regParam:\", bestModel._java_obj.getRegParam())\n",
    "print(\"Model Terbaik - elasticNetParam:\", bestModel._java_obj.getElasticNetParam())\n",
    "\n",
    "pred_best = bestModel.transform(df_final)\n",
    "best_accuracy = evaluator.evaluate(pred_best)\n",
    "\n",
    "print(\"Akurasi Terbaik Setelah Tuning:\", best_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Sabil)",
   "language": "python",
   "name": "sabil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
